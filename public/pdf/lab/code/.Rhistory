monotone_constraints = row$monotone_constraints
)
fold1 = sort(sample(1:nrow(df_f_train), replace=FALSE, size=0.5*nrow(df_f_train)))
fold2 = setdiff(1:nrow(df_f_train), fold1)
folds = list(Fold1 = fold1, Fold2 = fold2)
# do the cross validation
wp_cv_model <- xgboost::xgb.cv(
data = f_xgb_trainMat,
params = params,
folds = folds,
metrics = list("logloss"),
nrounds = 15000,
early_stopping_rounds = 50,
print_every_n = 50
)
# bundle up the results together for returning
output <- params
output$iter <- wp_cv_model$best_iteration
output$logloss <- wp_cv_model$evaluation_log[output$iter]$test_logloss_mean
row_result <- bind_rows(output)
return(row_result)
}
# get results
results = map_df(1:nrow(f_xgb_param_grid), function(x) {
print(paste0("row ", x)); return(get_row(f_xgb_param_grid %>% dplyr::slice(x)))
})
# visualize param tuning
results %>%
dplyr::select(logloss, eta, gamma, subsample, colsample_bytree, max_depth, min_child_weight) %>%
tidyr::pivot_longer(
eta:min_child_weight,
values_to = "value",
names_to = "parameter"
) %>%
ggplot(aes(value, logloss, color = parameter)) +
geom_point(alpha = 0.8, show.legend = FALSE, size = 3) +
facet_wrap(~parameter, scales = "free_x") +
labs(x = NULL, y = "logloss") +
theme_minimal()
# re-tune, with better param range based on these plots...
# Collect best parameters
results %>% arrange(logloss) %>% select(eta, subsample, colsample_bytree, max_depth, logloss, min_child_weight, iter)
best_model <- results %>% arrange(logloss) %>% slice_head()
best_model
params <- list(
booster = "gbtree",
objective = "reg:logistic",
# objective = "binary:logistic",
eval_metric = c("logloss"),
eta = best_model$eta,
gamma = best_model$gamma,
subsample = best_model$subsample,
colsample_bytree = best_model$colsample_bytree,
max_depth = best_model$max_depth,
min_child_weight = best_model$min_child_weight,
monotone_constraints = best_model$monotone_constraints,
nrounds = best_model$iter
)
params
list.save(params, "data/xgb_f_grid_params.yaml")
###############################################################
}
### get parameters xgboost model
params = list.load("data/xgb_f_grid_params.yaml")
params
### XGBoost model to f_grid matrix
xgb_to_grid <- function(f_xgb) {
max_r = 10
f_xgb_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix for XGBoost
for (inn in 1:9) {
test_df_inn = tibble(INNING = inn, CUM_RUNS = 0:(max_r))
f_xgb_grid[inn,] = pred_xgb_f_grid(test_df_inn, f_xgb)
}
rownames(f_xgb_grid) = paste0("INNING_",1:9)
colnames(f_xgb_grid) = paste0("CUM_RUNS_",(0:(ncol(f_xgb_grid)-1)))
return(f_xgb_grid)
}
### fit and plot the 2010-2019 XGBoost grid
xgb_f_grid_xgb = fit_xgb_f_grid(params, df_f_grid)
y
y = c(1,0,1,1,1,0)
p_hat = c(0.7, 0.2, 0.8, 0.9, 0.5, 0.25)
logloss
logloss <- function(y,p) { -y*log(p) - (1-y)*log(1-p) }
log(0.8)
-log(0.8)
-log(0.2)
xgb_f_grid_xgb
xgb_f_grid = xgb_to_grid(xgb_f_grid_xgb)
xgb_f_grid
plot_grid_f_IR(xgb_f_grid)
### fit and plot the 2019 NL XGBoost grid
fit_xgb_f_grid(
params, df_f_grid %>% filter(YEAR == 2019, HOME_LEAGUE == "NL")
)
plot_grid_f_IR(xgb_to_grid(fit_xgb_f_grid(
params, df_f_grid %>% filter(YEAR == 2019, HOME_LEAGUE == "NL")
)))
plot_grid_f_IR(xgb_to_grid(fit_xgb_f_grid(
params, df_f_grid %>% filter(YEAR == 2018, HOME_LEAGUE == "NL")
)))
plot_grid_f_IR(xgb_f_grid)
plot_grid_f_IR(xgb_to_grid(fit_xgb_f_grid(
params, df_f_grid %>% filter(YEAR == 2017, HOME_LEAGUE == "NL")
)))
df_lambda_yr_lg = df_f_grid %>%
group_by(YEAR,HOME_LEAGUE,BAT_TEAM_ID) %>%
summarise(lambda = mean(INN_RUNS)) %>%
group_by(YEAR,HOME_LEAGUE) %>%
summarise(lambda = mean(lambda))
df_f_grid
df_f_grid %>%
group_by(YEAR,HOME_LEAGUE,BAT_TEAM_ID) %>%
summarise(lambda = mean(INN_RUNS))
df_lambda_yr_lg = df_f_grid %>%
group_by(YEAR,HOME_LEAGUE,BAT_TEAM_ID) %>%
summarise(lambda = mean(INN_RUNS)) %>%
group_by(YEAR,HOME_LEAGUE) %>%
summarise(lambda = mean(lambda))
df_lambda_yr_lg
?pskellam
### Skellam f grid
get_f_grid_Skellam <- function(lambda, max_r = 10) {
f_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix
rownames(f_grid) = paste0("INNING_",1:9)
colnames(f_grid) = paste0("CUM_RUNS_",(0:(ncol(f_grid)-1)))
for (R in 0:max_r) {
for (I in 1:9) {
if (I < 9) {
t1 = pskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda, lower.tail = FALSE)
### pskellam means P(Skellam(9lambda, 9(9-I-1)) > R)
t2 = dskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda)
### dskellam means P(Skellam == R)
f_grid[I,R+1] = t1 + 1/2*t2
} else {
t1 = ppois(R, lambda = 9*lambda, lower.tail = FALSE)
t2 = dpois(R, lambda = 9*lambda)
f_grid[I,R+1] = t1 + 1/2*t2
}
}
}
return(f_grid)
}
### Skellam f grid
get_f_grid_Skellam <- function(lambda, max_r = 10) {
f_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix
rownames(f_grid) = paste0("INNING_",1:9)
colnames(f_grid) = paste0("CUM_RUNS_",(0:(ncol(f_grid)-1)))
for (R in 0:max_r) {
for (I in 1:9) {
if (I < 9) {
t1 = pskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda, lower.tail = FALSE)
### pskellam means P(Skellam(9lambda, 9(9-I-1)) > R)
t2 = dskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda)
### dskellam means P(Skellam == R)
f_grid[I,R+1] = t1 + 1/2*t2
} else {
t1 = ppois(R, lambda = 9*lambda, lower.tail = FALSE)
t2 = dpois(R, lambda = 9*lambda)
f_grid[I,R+1] = t1 + 1/2*t2
}
}
}
return(f_grid)
}
### fit Skellam f grids in each lg-szn
f_grids_Skellam = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
f_grid_yr_lg = get_f_grid_Skellam(lambda = lambda_yr_lg, max_r = 10)
# save Skellam grid
f_grids_Skellam[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
f_grids_Skellam
f_grids_Skellam
### visualize 2019 NL Skellam grid
plot_grid_f_IR(f_grids_Skellam[["f_grid_2019_NL"]])
f_grids_Skellam[["f_grid_2019_NL"]]
### Skellam f grid
get_f_grid_Skellam <- function(lambda, max_r = 10) {
f_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix
rownames(f_grid) = paste0("INNING_",1:9)
colnames(f_grid) = paste0("CUM_RUNS_",(0:(ncol(f_grid)-1)))
for (R in 0:max_r) {
for (I in 1:9) {
if (I < 9) {
t1 = pskellam(R, lambda1 = 9*lambda, lambda2 = (9-I-1)*lambda, lower.tail = FALSE)
### pskellam means P(Skellam(9lambda, 9(9-I-1)) > R)
t2 = dskellam(R, lambda1 = 9*lambda, lambda2 = (9-I-1)*lambda)
### dskellam means P(Skellam == R)
f_grid[I,R+1] = t1 + 1/2*t2
} else {
t1 = ppois(R, lambda = 9*lambda, lower.tail = FALSE)
t2 = dpois(R, lambda = 9*lambda)
f_grid[I,R+1] = t1 + 1/2*t2
}
}
}
return(f_grid)
}
### fit Skellam f grids in each lg-szn
f_grids_Skellam = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
f_grid_yr_lg = get_f_grid_Skellam(lambda = lambda_yr_lg, max_r = 10)
# save Skellam grid
f_grids_Skellam[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
### visualize 2019 NL Skellam grid
plot_grid_f_IR(f_grids_Skellam[["f_grid_2019_NL"]])
### Skellam f grid
get_f_grid_Skellam <- function(lambda, max_r = 10) {
f_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix
rownames(f_grid) = paste0("INNING_",1:9)
colnames(f_grid) = paste0("CUM_RUNS_",(0:(ncol(f_grid)-1)))
for (R in 0:max_r) {
for (I in 1:9) {
if (I < 9) {
t1 = pskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda, lower.tail = FALSE)
### pskellam means P(Skellam(9lambda, 9(9-I-1)) > R)
t2 = dskellam(R, lambda1 = 9*lambda, lambda2 = (9-I)*lambda)
### dskellam means P(Skellam == R)
f_grid[I,R+1] = t1 + 1/2*t2
} else {
t1 = ppois(R, lambda = 9*lambda, lower.tail = FALSE)
t2 = dpois(R, lambda = 9*lambda)
f_grid[I,R+1] = t1 + 1/2*t2
}
}
}
return(f_grid)
}
### fit Skellam f grids in each lg-szn
f_grids_Skellam = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
f_grid_yr_lg = get_f_grid_Skellam(lambda = lambda_yr_lg, max_r = 10)
# save Skellam grid
f_grids_Skellam[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
### visualize 2019 NL Skellam grid
plot_grid_f_IR(f_grids_Skellam[["f_grid_2019_NL"]])
df_lambda_yr_lg = df_f_grid %>%
group_by(YEAR,HOME_LEAGUE,BAT_TEAM_ID) %>%
summarise(
lambda = mean(INN_RUNS),
sigma = sd(INN_RUNS)
) %>%
group_by(YEAR,HOME_LEAGUE) %>%
summarise(
lambda = mean(lambda),
sigma = mean(sigma)
)
df_lambda_yr_lg
###
get_f_grid_Skellam_XY <- function(lambda_X, lambda_Y, max_r = 10) {
f_grid = matrix(nrow = 9, ncol = max_r+1) ### WP matrix
rownames(f_grid) = paste0("INNING_",1:9)
colnames(f_grid) = paste0("CUM_RUNS_",(0:(ncol(f_grid)-1)))
for (R in 0:max_r) {
for (I in 1:9) {
if (I < 9) {
t1 = pskellam(R, lambda1 = 9*lambda_X, lambda2 = (9-I)*lambda_Y, lower.tail = FALSE)
t2 = dskellam(R, lambda1 = 9*lambda_X, lambda2 = (9-I)*lambda_Y)
f_grid[I,R+1] = t1 + 1/2*t2
} else {
t1 = ppois(R, lambda = 9*lambda_X, lower.tail = FALSE)
t2 = dpois(R, lambda = 9*lambda_X)
f_grid[I,R+1] = t1 + 1/2*t2
}
}
}
return(f_grid)
}
### Skellam f grid with positive Normal prior via Monte Carlo Simulation
monte_carlo_f_grid <- function(lambda_hat, sigma_hat, B=100) {
f_grid = NULL
for (b in 1:B) {
browser()
### sample lambda_X and lambda_Y
lambda_X_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_Y_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
### get f(I,R) grid for this league-season and monte carlo sample b
f_grid_b = get_f_grid_Skellam_XY(lambda_X_b, lambda_Y_b, max_r = 10)
if (b == 1) {
### initialize the grid f(I,R | yr,lg)
f_grid = f_grid_b
} else {
### running average of the grid
f_grid = 1/b * f_grid_b + (b-1)/b * f_grid
}
}
return(f_grid)
}
### fit Skellam f grids with positive Normal prior in each lg-szn
f_grids_Skellam_pN = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
print(paste0("computing f(I,R) grid for lg ", lg, " and szn ", yr))
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
sigma_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$sigma
f_grid_yr_lg = monte_carlo_f_grid(lambda_yr_lg, sigma_yr_lg)
### save Skellam grid
f_grids_Skellam_pN[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
lambda_hat
sigma_hat
B
b
lambda_X_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_X_b
lambda_hat
sigma_hat
lambda_X_b
lambda_Y_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_Y_b
### sample lambda_X and lambda_Y
lambda_X_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_Y_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_X_b
lambda_Y_b
lambda_X_b
lambda_Y_b
f_grid_b = get_f_grid_Skellam_XY(lambda_X_b, lambda_Y_b, max_r = 10)
f_grid_b
### Skellam f grid with positive Normal prior via Monte Carlo Simulation
monte_carlo_f_grid <- function(lambda_hat, sigma_hat, B=100) {
f_grid = NULL
for (b in 1:B) {
# browser()
### sample lambda_X and lambda_Y
lambda_X_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
lambda_Y_b = truncnorm::rtruncnorm(n=1, a=0, mean=lambda_hat, sd=sigma_hat)
### get f(I,R) grid for this league-season and monte carlo sample b
f_grid_b = get_f_grid_Skellam_XY(lambda_X_b, lambda_Y_b, max_r = 10)
if (b == 1) {
### initialize the grid f(I,R | yr,lg)
f_grid = f_grid_b
} else {
### running average of the grid
f_grid = 1/b * f_grid_b + (b-1)/b * f_grid
}
}
return(f_grid)
}
### fit Skellam f grids with positive Normal prior in each lg-szn
f_grids_Skellam_pN = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
print(paste0("computing f(I,R) grid for lg ", lg, " and szn ", yr))
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
sigma_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$sigma
f_grid_yr_lg = monte_carlo_f_grid(lambda_yr_lg, sigma_yr_lg)
### save Skellam grid
f_grids_Skellam_pN[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
### visualize 2019 NL Skellam grid with positive Normal prior
plot_grid_f_IR(f_grids_Skellam_pN[["f_grid_2019_NL"]])
f_grids_Skellam_pN
### visualize 2019 NL Skellam grid with positive Normal prior
plot_grid_f_IR(f_grids_Skellam_pN[["f_grid_2019_NL"]])
df_lambda_yr_lg
### tune k for the fit dispersed Skellam f(I,R)
lambdaF = mean(df_f_grid$INN_RUNS)
sigmaF = sd(df_f_grid$INN_RUNS)
# choose the k which minimizes the logloss of
# f grid predictions and the actual win/loss column
set.seed(22) #Kershaw
# ks = seq(0.1,1,by=0.05)
ks = seq(0.2,0.3,by=0.01)
ks
seq(0.1,1,by=0.05)
seq(0.2,0.3,by=0.01)
ks = seq(0.1,1,by=0.05)
ks
ks
logLosses = numeric(length(ks))
i=5
k = ks[i]
k
lambdaF
sigmaF
f_grid_k = monte_carlo_f_grid(lambdaF, sigmaF*k)
f_grid_k
f_grid_k
f_grid_k1 = reshape2::melt(f_grid_k) %>%
mutate(
INNING = as.numeric(str_sub(Var1, start=8)),
CUM_RUNS = as.numeric(str_sub(Var2, start=10)),
) %>%
rename(wp_hat = value) %>%
select(-c(Var1,Var2)) %>%
as_tibble()
eval_df_k = (df_f_grid %>%
left_join(f_grid_k1) %>%
summarise(logloss_ = logloss(PIT_WINS, wp_hat)))$logloss_
logLosses[i] = eval_df_k
logLosses
# choose the k which minimizes the logloss of
# f grid predictions and the actual win/loss column
set.seed(22) #Kershaw
# ks = seq(0.1,1,by=0.05)
ks = seq(0.2,0.3,by=0.01)
logLosses = numeric(length(ks))
for (i in 1:length(ks)) {
k = ks[i]
f_grid_k = monte_carlo_f_grid(lambdaF, sigmaF*k)
f_grid_k1 = reshape2::melt(f_grid_k) %>%
mutate(
INNING = as.numeric(str_sub(Var1, start=8)),
CUM_RUNS = as.numeric(str_sub(Var2, start=10)),
) %>%
rename(wp_hat = value) %>%
select(-c(Var1,Var2)) %>%
as_tibble()
eval_df_k = (df_f_grid %>%
left_join(f_grid_k1) %>%
summarise(logloss_ = logloss(PIT_WINS, wp_hat)))$logloss_
logLosses[i] = eval_df_k
}
plot(logLosses)
# choose the k which minimizes the logloss of
# f grid predictions and the actual win/loss column
set.seed(22) #Kershaw
# ks = seq(0.1,1,by=0.05)
ks = seq(0.2,0.3,by=0.01)
logLosses = numeric(length(ks))
for (i in 1:length(ks)) {
k = ks[i]
f_grid_k = monte_carlo_f_grid(lambdaF, sigmaF*k)
f_grid_k1 = reshape2::melt(f_grid_k) %>%
mutate(
INNING = as.numeric(str_sub(Var1, start=8)),
CUM_RUNS = as.numeric(str_sub(Var2, start=10)),
) %>%
rename(wp_hat = value) %>%
select(-c(Var1,Var2)) %>%
as_tibble()
eval_df_k = (df_f_grid %>%
left_join(f_grid_k1) %>%
summarise(logloss_ = logloss(PIT_WINS, wp_hat)))$logloss_
logLosses[i] = eval_df_k
}
plot(logLosses)
# ks = seq(0.1,1,by=0.05)
ks = seq(0.2,0.3,by=0.01)
ks
logLosses = numeric(length(ks))
for (i in 1:length(ks)) {
k = ks[i]
f_grid_k = monte_carlo_f_grid(lambdaF, sigmaF*k)
f_grid_k1 = reshape2::melt(f_grid_k) %>%
mutate(
INNING = as.numeric(str_sub(Var1, start=8)),
CUM_RUNS = as.numeric(str_sub(Var2, start=10)),
) %>%
rename(wp_hat = value) %>%
select(-c(Var1,Var2)) %>%
as_tibble()
eval_df_k = (df_f_grid %>%
left_join(f_grid_k1) %>%
summarise(logloss_ = logloss(PIT_WINS, wp_hat)))$logloss_
logLosses[i] = eval_df_k
}
plot(logLosses)
# choose the k which minimizes the logloss of
# f grid predictions and the actual win/loss column
set.seed(22) #Kershaw
# ks = seq(0.1,1,by=0.05)
ks = seq(0.2,0.3,by=0.01)
logLosses = numeric(length(ks))
for (i in 1:length(ks)) {
k = ks[i]
f_grid_k = monte_carlo_f_grid(lambdaF, sigmaF*k)
f_grid_k1 = reshape2::melt(f_grid_k) %>%
mutate(
INNING = as.numeric(str_sub(Var1, start=8)),
CUM_RUNS = as.numeric(str_sub(Var2, start=10)),
) %>%
rename(wp_hat = value) %>%
select(-c(Var1,Var2)) %>%
as_tibble()
eval_df_k = (df_f_grid %>%
left_join(f_grid_k1) %>%
summarise(logloss_ = logloss(PIT_WINS, wp_hat)))$logloss_
logLosses[i] = eval_df_k
}
plot(logLosses)
ks[which(logLosses == min(logLosses))]
### fit Skellam f grids with positive Normal prior and tuned overdispersion in each lg-szn
f_grids_Skellam_pNk = list()
for (yr in unique(df_f_grid$YEAR)) {
for (lg in unique(df_f_grid$HOME_LEAGUE)) {
print(paste0("computing f(I,R) grid for lg ", lg, " and szn ", yr))
lambda_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$lambda
sigma_yr_lg = (df_lambda_yr_lg %>% filter(YEAR == yr & HOME_LEAGUE == lg))$sigma
k = 0.28 # overdispersion parameters
f_grid_yr_lg = monte_carlo_f_grid(lambda_yr_lg, sigma_yr_lg*k)
### save Skellam grid
f_grids_Skellam_pNk[[paste0("f_grid_",yr,"_",lg)]] = f_grid_yr_lg
}
}
### visualize 2019 NL Skellam grid with positive Normal prior and tuned overdispersion
plot_grid_f_IR(f_grids_Skellam_pNk[["f_grid_2019_NL"]])
